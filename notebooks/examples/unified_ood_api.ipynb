{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e39fdfb",
   "metadata": {},
   "source": [
    "# A Brief Introduction to probly’s Unified OOD Evaluation API\n",
    "\n",
    "This notebook introduces the **unified out-of-distribution (OOD) evaluation API** provided by `probly`. It explains the motivation, core concepts, and advanced usage patterns behind the `evaluate_ood` function, and shows how it enables **clean, extensible, and backward-compatible** OOD evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Motivation\n",
    "\n",
    "Evaluating out-of-distribution detection methods typically involves multiple metrics:\n",
    "\n",
    "* AUROC\n",
    "* AUPR\n",
    "* FPR or FNR at a fixed TPR\n",
    "\n",
    "In practice, these metrics are often exposed via **separate functions**, with different calling conventions and limited extensibility. This leads to brittle evaluation code and makes experimentation harder.\n",
    "\n",
    "The goal of probly’s unified OOD evaluation API is to:\n",
    "\n",
    "* Provide **one entry point** for all common OOD metrics\n",
    "* Preserve **backward compatibility** with existing AUROC-based workflows\n",
    "* Allow **easy extension** with new metrics and thresholds\n",
    "* Support **configuration-friendly metric specifications** (e.g. strings)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Hello World: AUROC (Backward Compatible)\n",
    "\n",
    "We start with the simplest possible use case: computing AUROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab4569cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.925721\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from probly.evaluation.ood_api import evaluate_ood\n",
    "\n",
    "in_scores = np.random.normal(0, 1, size=1000)\n",
    "out_scores = np.random.normal(2, 1, size=1000)\n",
    "\n",
    "auroc = evaluate_ood(in_scores, out_scores)\n",
    "print(\"AUROC:\", auroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f13b8",
   "metadata": {},
   "source": [
    "### What is happening here?\n",
    "\n",
    "* `metrics=None` is the default\n",
    "* The function returns a **single float**\n",
    "* Existing code that expects `evaluate_ood(...) -> float` continues to work unchanged\n",
    "\n",
    "This ensures **full backward compatibility**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Requesting Multiple Metrics\n",
    "\n",
    "OOD evaluation rarely relies on a single metric. The unified API allows requesting multiple metrics at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816b9d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc: 0.9218\n",
      "aupr: 0.9212\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_ood(\n",
    "    in_scores,\n",
    "    out_scores,\n",
    "    metrics=[\"auroc\", \"aupr\"],\n",
    ")\n",
    "\n",
    "for name, value in results.items():\n",
    "    print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d135f15",
   "metadata": {},
   "source": [
    "### Design choice\n",
    "\n",
    "* Input: list of metric specifications\n",
    "* Output: dictionary mapping metric name → value\n",
    "* Uniform interface for all metrics\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Exploring All Available Metrics\n",
    "\n",
    "For exploratory analysis or benchmarking, it is often useful to compute *all* supported metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba3afc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auroc': 0.921818, 'aupr': 0.9212099095167594}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_ood(\n",
    "    in_scores,\n",
    "    out_scores,\n",
    "    metrics=\"all\",\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4abd38",
   "metadata": {},
   "source": [
    "This returns a dictionary containing:\n",
    "\n",
    "* AUROC\n",
    "* AUPR\n",
    "* FPR @ 95% TPR\n",
    "* FNR @ 95% TPR\n",
    "\n",
    "The exact set can grow over time without breaking the API.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Metrics at a Glance\n",
    "\n",
    "**AUROC** measures how well the score separates ID from OOD samples independent of a fixed threshold. Values close to 1 indicate strong separation, while 0.5 corresponds to random guessing.\n",
    "\n",
    "**AUPR** is especially useful when OOD samples are rare, as it emphasizes whether high scores truly correspond to OOD examples. Values close to 1 and clearly above the OOD base rate indicate clean and reliable alarms.\n",
    "\n",
    "**FPR@XTPR (e.g. FPR@95TPR)** quantifies how many ID samples are falsely flagged as OOD when a desired OOD detection rate is enforced. Lower values mean fewer false alarms on ID data.\n",
    "\n",
    "**FNR@XTPR (e.g. FNR@95TPR)** measures how many OOD samples are still missed at a given detection level. Lower values indicate that most OOD cases are successfully detected.\n",
    "\n",
    "--------------|--------|\n",
    "| `fpr` | FPR at default TPR (95%) |\n",
    "| `fpr@0.8` | FPR at 80% TPR |\n",
    "| `fnr@95%` | FNR at 95% TPR |\n",
    "\n",
    "This design makes the API:\n",
    "\n",
    "* Easy to use from configuration files\n",
    "* Friendly to CLI tools\n",
    "* Self-documenting\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Static vs. Dynamic Metrics\n",
    "\n",
    "Internally, the API distinguishes between **static** and **dynamic** metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10452875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probly.evaluation.tasks import (\n",
    "    out_of_distribution_detection_aupr,\n",
    "    out_of_distribution_detection_auroc,\n",
    "    out_of_distribution_detection_fnr_at_x_tpr,\n",
    "    out_of_distribution_detection_fpr_at_x_tpr,\n",
    ")\n",
    "\n",
    "STATIC_METRICS = {\n",
    "    \"auroc\": out_of_distribution_detection_auroc,\n",
    "    \"aupr\": out_of_distribution_detection_aupr,\n",
    "}\n",
    "\n",
    "DYNAMIC_METRICS = {\n",
    "    \"fpr\": out_of_distribution_detection_fpr_at_x_tpr,\n",
    "    \"fnr\": out_of_distribution_detection_fnr_at_x_tpr,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c568279",
   "metadata": {},
   "source": [
    "### Why this separation?\n",
    "\n",
    "* Static metrics depend only on scores\n",
    "* Dynamic metrics additionally depend on a threshold\n",
    "* New metrics can be added by extending these dictionaries\n",
    "\n",
    "No changes to `evaluate_ood` itself are required.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Parsing Metric Specifications\n",
    "\n",
    "Dynamic metrics are parsed using a small helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88e9fb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fnr 0.9\n"
     ]
    }
   ],
   "source": [
    "from probly.evaluation.ood_api import parse_dynamic_metric\n",
    "\n",
    "base, threshold = parse_dynamic_metric(\"fnr@90%\")\n",
    "print(base, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24cf96",
   "metadata": {},
   "source": [
    "The parser supports:\n",
    "\n",
    "* Percent values (`90%`)\n",
    "* Floating point values (`0.9`)\n",
    "* Default thresholds when omitted\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Robust Error Handling\n",
    "\n",
    "Invalid metric specifications fail early with clear error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac0f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown metric 'foo'. Available: ['auroc', 'aupr'] + dynamic metric@value.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    evaluate_ood(in_scores, out_scores, metrics=[\"foo\", \"fpr@2.0\"])\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1b09b",
   "metadata": {},
   "source": [
    "### Design principles\n",
    "\n",
    "* Fail fast\n",
    "* Explicit validation\n",
    "* No silent defaults\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Best Practices\n",
    "\n",
    "**Recommended**\n",
    "\n",
    "* Use `\"auroc\"` for quick comparisons\n",
    "* Use `\"all\"` for exploratory evaluation\n",
    "* Use explicit thresholds (`fpr@95%`) in benchmarks\n",
    "\n",
    "**Avoid**\n",
    "\n",
    "* Hard-coded metric functions scattered across codebases\n",
    "* Implicit thresholds without documentation\n",
    "* Mixing multiple evaluation APIs\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "The unified OOD evaluation API in probly provides:\n",
    "\n",
    "* A single, consistent entry point\n",
    "* Backward compatibility with legacy workflows\n",
    "* Easy extensibility for new metrics\n",
    "* Clear, configuration-friendly semantics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
